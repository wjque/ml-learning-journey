# 线性代数复习

## 矩阵

- 矩阵与向量的乘法：是矩阵列向量的线性组合；
- 矩阵的分解
  - 方阵 —— 特征分解：将**方阵**分解为一组特征向量和特征值，$Av = \lambda v$，一般指的是右特征向量
    - 特征向量的特点是，矩阵左乘特征向量的效果是将特征向量进行一定比例的缩放；
    - 特征值的意义便是矩阵左乘特征向量后，特征向量被缩放的倍数，由于倍数可变，所以一般考虑的都是单位特征向量;
    - 所有特征向量作为列向量构成的矩阵为 $V$，若 $V$ 为可逆方阵，那么 $AV = Vdiag(\lambda_i)$，所以 $A = Vdiag(\lambda_i)V^{-1}$
    - 性质：
      - 每个实对称矩阵都能分解成实特征向量和实特征值：$A = Q\Lambda Q^T$
      - 矩阵是奇异的，当且仅当含有零特征值
      - $f(x) = x^T A x$ 的最大值是最大特征值，最小值是最小特征值，由此可以定义矩阵的正定性
  - 一般矩阵 —— 奇异值分解：$A = U D V^T$，若 $A$ 为 $m \times n$，那么 $U$ 为 $m\times m$，$D$ 为 $m \times n$，$V$ 为 $n \times n$
    - $U$ 和 $V$ 定义为正交矩阵，$D$ 定义为对角矩阵（不一定是方阵）
    - 如何计算 —— **IMPORTANT**
      - $U$ 的列向量是矩阵 $AA^T$ 的特征向量
      - $V$ 的行向量是矩阵 $A^TA$ 的特征向量
      - $U$ 对角线元素与特征值的关系：$d_i = \sqrt{\lambda_i}$
    - 求 $Moore-Penrose$ 广义逆：$A^+ = VD^+U^T$，其中 $D^+$ 是矩阵 $D$ 所有非零元素取倒数后再取转置
      - 性质：当 $A$ 的列数小于行数时，$x = A^+y$ 是方程 $Ax = y$ 的一个可行解，且是所有可行解中**二范数最小的**；当 $A$ 的列数大于行数时，$Ax = y$ 可能没有可行解，但是 $x = A^+y$ 是使得欧几里得距离 $||y - Ax||$ 最小的解（也即最小二乘解）
- 行列式（针对方阵）：矩阵所有特征值的乘积
  
## 主成分分析(PCA)

- 问题描述：面对高维数据 $X \in \mathbb{R}^{n\times d}$，其不同维度之间可能存在相关关系，使用主成分分析的方法可以消除这种相关性，**在损失尽量少信息的前提下实现数据降维**
- 原理 —— **最小化“重构误差”**：
  - 我们希望找到一个 $\mathbb{R}^n \rightarrow \mathbb{R}^k$ 的编码映射 $f(x)$，以及一个 $\mathbb{R}^k \rightarrow \mathbb{R}^n$ 的解码映射 $g(x)$，记 $\hat{x_i} = g(f(x_i))$，使得 $\sum \limits_{i} ||x_i - \hat{x_i}||^2$ 最小，那么 $f(X)$ 便是降维后的数据
  - 使用矩阵作为编码映射可以消除维度之间的线性关系，同理若选择非线性函数作为编码函数，那么最后可以消除维度之间的非线性关系
  - 具体操作步骤 —— **IMPORTANT**
    - 为了消除量纲的影响，将所有特征（$X$ 的列向量）进行标准化；
    - 计算协方差矩阵：$C = \frac{1}{n-1}X^TX \in \mathbb{R}^{d \times d}$
    - 计算协方差矩阵（实对称）的特征值与特征向量 $C = Q\Lambda Q^T$
      - 得到 $d$ 个特征值：$\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_d$
      - 对应的特征向量： $q_1,q_2,...,q_d$
    - 选择**前 $k$ 个最大的特征值对应的特征向量**，构成编码矩阵 $W\in \mathbb{R}^{d \times k}$，将原始数据映射到新空间：$Z = XW \in \mathbb{R}^{n \times k}$
