# 概率论与信息论复习

## 概率论

要有“多元思维”，机器学习中使用的变量通常维数很高，在学习概率论的时候多注意高维分布的性质

矩母函数：$\phi_X(u) = \mathbb{E}[e^{uX}]$

特征函数：$\phi_X(\lambda) = \mathbb{E}[e^{i\lambda X}]$

随机变量函数的分布：链式法则，求 $Jaccobi$ 矩阵行列式的绝对值

- 独立性的判断：
  - 联合分布等于边缘分布之积：$P(X,Y)=P(X)P(Y)$
  - 联合密度函数可以拆分为边缘密度函数之积：$f_{X,Y}(x,y) = f_X(x)f_Y(y)$
  - 联合特征函数可分解：$\phi_{X,Y}(s,t) = \phi_X(s)\phi_Y(t)$
  - 实际中使用的是 **Hilbert-Schmidt Independence Criterion**，一种基于核方法的非参数独立性度量
- 协方差的计算：
  - 向量的协方差矩阵，形式与单变量相同：$Cov(\bold{x},\bold{y})=\mathbb{E}[(\bold{x}-\mathbb{E}\bold{x})(\bold{y}-\mathbb{E}\bold{y})]$
  - 矩阵的协方差矩阵：$X_{centered}=X - \frac{1}{n}\bold{1}_n\bold{1}^T_n X,Y_{centered}=Y-\frac{1}{n}\bold{1}_n \bold{1}^T_n Y,Cov(X, Y)=\frac{1}{n-1}X_{centered}\cdot Y_{centered}$，每一列减去这一列的均值
- 常用概率分布：
  - Bernoulli 分布
  - Multinoulli 分布
  - 高斯分布
    - 可加性：若 $X_1,X_2,...,X_n$ **相互独立**或者**联合分布为 Gauss**，那么其线性组合仍为 Gauss
    - 不相关与独立等价：若**联合分布为 Gauss**，那么它们不相关与它们独立是等价的
    - 条件分布为 Gauss：若 $(X_1,X_2)\sim N(\mu, \Sigma)$（自己分块吧，不好打出来），那么其条件分布 $X_1|X_2=x_2 \sim N(\mu_{1|2}, \Sigma_{1|2})$，其中 $\mu_{1|2} = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2),\Sigma_{1|2}=\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}$
    - 联合分布为 Gauss 的随机变量 $X_1, X_2$，用 $X_2$ 预测 $X_1$ 的最优预测 $\mathbb{E}(X_1|X_2)$ 为线性预测（见上一条性质）
  - 指数分布和 Laplace 分布
    - 指数分布：将最高点设定在 $x=0$ 处；
    - Laplace 分布：可以将最高点设定在任意位置；
  - Dirac 分布和经验分布
    - Dirac 分布：对连续型随机变量而言，规定取得某一点的概率为1，密度函数取值为 $\infty$
    - 经验分布：设有 n 个样本，那么经验分布就是将概率均匀分布在这 n 个样本中，每一点的概率为 $\frac{1}{n}$，但是密度函数的取值仍为 $\infty$，相当于是 n 个 Dirac 分布的和
  - 混合分布
    - $P(\bold{x})=\sum \limits_i P(c=i)P(\bold{x}|c=i)$
    - 若 $P(\bold{x}|c=i)$ 是 Gauss 分布，这个混合分布称为高斯混合模型，高斯混合模型是概率密度的“**万能近似器**”，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度逼近
- 常用函数
  - logistic sigmoid function
    - $\sigma(x)=\frac{1}{1+\exp(-x)}=\frac{\exp(x)}{\exp(x)+\exp(0)}$
    - 性质：
      - $\frac{d}{dx}\sigma(x) = \sigma(x)(1-\sigma(x))=\sigma(x)\sigma(-x)$
      - 反函数 $\forall x\in (0,1),\sigma^{-1}(x) = \log(\frac{x}{1-x})$
  - softplus function
    - $\zeta(x)=\log(1+\exp(x)) $，是 ReLU 函数的平滑近似
    - 性质：
      - $\frac{d}{dx}\zeta(x)=\sigma(x)$
      - $\zeta(x) - \zeta(-x) = x$，这与一般定义的实数的正部和负部吻合

## 信息论

### 信息量的度量 —— 香农熵

- 一个事件发生的概率越小，那么我们认为它的发生蕴含的信息量就越大
- 独立事件同时发生蕴含的信息量应当是两个事件信息量的和

综合以上两点，我们定义事件 $\bold{x} = x$ 的自信息 $I(x)=-\log P(x)$，底数可以选择自然对数 e 或者 2，以 e 为底时，香农熵的单位为奈特(nats)，以 2 为底时，香农熵的单位为比特(bit)，奈特和比特只是常数倍的关系

如何定义一个分布的信息量？

- 我们使用香农熵定义分布的信息量：$H(\bold{x})=\mathbb{E}_{\bold{x}\sim P}[I(x)]=-\mathbb{E}_{\bold{x}\sim P}[\log P(x)]$
- 我们可以验证，那些接近确定性的概率分布具有较低的香农熵（例如发生概率接近 1 或接近 0 的 Bernoulli 分布），那些接近均匀分布的概率分布具有较高的香农熵

### 分布差异的描述 —— KL 散度

若一个随机变量服从两个单独的分布 P(x) 和 Q(x)，可以使用 KL 散度衡量这两个分布的差异：

$$D_{KL}(P||Q) = \mathbb{E}_{\bold{x}\sim P}[\log\frac{P(x)}{Q(x)}]$$

它通常用来描述分布之间的某种“距离”，但是它并不是真正的距离，因为它是非对称的：对于某些 P 和 Q，$D_{KL}(P||Q)\not ={D_{KL}(Q||P)}$

- 还有一个和 KL 散度关系密切的量是交叉熵(cross-entropy)，$H(P,Q)=H(P)+D_{KL}(P||Q)=-\mathbb{E}_{\bold{x}\sim P}[\log Q(x)]$

### 结构化概率模型 —— 有向图与无向图模型

大多数机器学习的任务涉及的参数维度都是非常高的，如果直接写出它们服从的联合分布是非常低效且不必要的，我们通常将它们划分为几个不同的模型，用模型之间的关系去描述最终任务的分布

- 有向图：利用条件概率来表示分解，箭头表示的关系是条件关系，箭头始端是箭头末端事件发生的条件：

![有向图模型](.\\pics\\directed_model.jpg)

- 无向图：使用函数表示分解，图中任意两两相互连边的顶点集合被称为团，每个团 $C^{(i)}$ 都伴随一个因子 $\phi^{(i)}(C^{(i)})$，这些因子仅仅是函数，而非概率分布，所以没有归一化限制，为了保证由这些因子组合成的概率分布有归一化性质，需要引入一个归一化系数 $Z$：

![无向图模型](.\\pics\\undirected_model.jpg)
