# 数值计算

## 上溢和下溢

上溢是指数据过大导致变为 `NAN` (not a number)；下溢指的是数据过小导致被识别为 0，这个数如果作为除数则会导致结果变为 `NAN`

以 softmax 函数为例：$softmax(x_i)=\frac{\exp(x_i)}{\sum \limits_j \exp(x_j)}$，若 $x_i$ 很大，则 $\exp(x_i)$ 就会变为 `NAN`，反之，若 $x_i$ 为一个很小的负数，则 softmax 的分母会变为 0，使得结果变为 `NAN`

解决这一问题的方法是令 $z_i = x_i - \max \limits_j x_j$，将 softmax 改写为 $softmax(x_i)=softmax(z_i)=\frac{\exp(z_i)}{\sum \limits_j \exp(z_j)}$，这样既可以保证 $z_i$ 不会过大，也可以保证分母不为 0 (因为至少有一个exp(0))

> python 库 `Theano` 里的函数能够自动识别可能发生的数值上溢或下溢并进行修正

## 病态条件

**条件数**描述的是输入发生微小变化时，输出结果可能发生的最大改变，用于衡量一个函数对输入变化的敏感性

例如当我们在求解方程：$\bold{Ax=b}$，若 $\bold{A}$ 可逆，则 $\bold{x=A^{-1}b}$，如果我们得到的 $\bold{b}$ 存在一定的误差，我们希望知道用这个方法求得的 $\bold{x}$ 尽量接近真实值，判断的方法便是用条件数

上述方程中，若 $\bold{A}$ 存在特征分解，我们定义 $\bold{A}$ 的条件数为 $\kappa(\bold{A})=\max \limits_{i,j} \frac{\lambda_i}{\lambda_j}$，即 $\bold{A}$ 最大特征值与最小特征值之比。条件数越大，意味着输出对输入越敏感，此时称矩阵 $\bold{A}$ 为病态矩阵

## 基于梯度的优化方法

总是沿着负梯度的方向优化代价函数，这被称为最速下降法 or 梯度下降法

- Jacobian 矩阵
  - 向量值函数 $\bold{y}=f(\bold{x}),\mathbb{R^n\rightarrow R^m}$ 的 Jacobian 矩阵计算方式：$J(\bold{f})_{ij}=\frac{\partial y_i}{\partial x_j}$
- Hessian 阵列（注意不一定是二维的，可能是三维张量）
  - 向量值函数 $\bold{y}=f(\bold{x}),\mathbb{R^n\rightarrow R^m}$ 的 Hessian 阵列计算方式：$H(\bold{f})(\bold{x_0})_{ij}=\frac{\partial^2 \bold{f}}{\partial x_i \partial x_j}|_{\bold{x=x_0}}$

一阶梯度下降法（未引入 Hessian 的曲率信息）的缺点：

1. 学习率难以设定
2. 收敛速度慢（线性收敛）
3. 对病态条件敏感，**更新方向只是局部最优**，会出现“之”字形低效优化；

**二阶梯度下降法（E.X. 牛顿法）**：

## 约束优化

问题描述：我们希望 $\min \limits_{\bold{x}} \bold{f(x)},\ s.t.\ g_i(\bold{x})=0,\forall i=0,1,...,m \And h_j(\bold{x})\le 0,\forall j=0,1,...,p$

- KKT 方法
  - 是 Lagrange 乘子法的推广，适用于不等式约束的情形
  - 约束条件：$\mathbb{S}=\{\bold{x}|\forall i,j,\ g_i(\bold{x})=0\ \&\ h_j(\bold{x})\le 0\}$
  - 操作步骤：
    - 定义广义 Lagrange 函数：$\mathcal{L(\bold{x},\boldsymbol{\lambda},\boldsymbol{\alpha})}=f\bold{(x)}+\sum \limits_i \lambda_i g_i\bold{(x)}+\sum \limits_j \alpha_j h_j\bold{(x)}$，我们的目标变为了：$\min \limits_{\bold{x}} \max \limits_{\boldsymbol{\lambda}} \max \limits_{\boldsymbol{\alpha}} \mathcal{L(\bold{x},\boldsymbol{\lambda},\boldsymbol{\alpha})}$
    - 写出 KKT 条件，对于一个最优解 $\bold{x^*}$ 及其对应的乘子 $\boldsymbol{\lambda}^*, \boldsymbol{\alpha}^*$，对它们作如下限制：
      - 原始可行性：$g_i(\bold{x^*})=0\ \And h_j(\bold{x^*})\le 0$
      - 对偶可行性：$\forall\alpha_j^* \ge 0$，确保沿着正确的方向惩罚
      - 互补松弛条件：$\boldsymbol{\alpha} \bold{h(x)}=0$
      - 广义 Lagrange 函数关于 $\bold{x}$ 的梯度为 0：$\nabla_{\bold{x}}\mathcal{L}(\bold{x^*},\boldsymbol{\lambda}^*,\boldsymbol{\alpha}^*)=0$，i.e. $\nabla_{\bold{x}}f(\bold{x})+\sum \limits_i \lambda_i \nabla_{\bold{x}}g_i(\bold{x})+\sum \limits_j \alpha_j \nabla_{\bold{x}}h_j(\bold{x})=0$
      - 需要注意的是，以上条件只是必要条件而非充分条件，求出来的解不一定都是最优解
