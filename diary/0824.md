# 机器学习基础

## 学习算法

对于某类任务 `T` 和性能度量 `P`，一个计算机程序被认为能够从经验 `E` 中学习是指，经过经验 `E` 的修正后，该程序在性能度量 `P` 上表现的性能有所提升

这里 `T`、`P`、`E` 的概念实际上非常宽泛，我们不去具体解释它们，而在之后列举出的任务中理解机器学习算法如何运行

通常根据 `E` 中数据是否有人为定义的标签，将机器学习算法分为监督学习 (supervised learning) 和无监督学习 (unsupervised learning)

- 监督学习解决的任务包括：分类、回归、结构化输出（如序列转录、语法分析等等）；
- 无监督学习解决的任务包括：密度估计和概率质量估计（如聚类 `Clustering`、嵌入 `Embedding` 等等）

## 容量、欠拟合与过拟合

机器学习算法最重要的一个功能就是**泛化**：我们不仅希望模型的训练误差较小，更希望它在未接触过数据上的泛化误差（也称测试误差）也可以尽可能小

如何合理地估计泛化误差？

- 前提假设：训练集和测试集的样本满足**独立同分布**，这个分布被称为**数据生成分布**；
- 在上述假设下，我们可以从数学上分析训练误差和泛化误差之间的关系；

如果模型的训练误差始终较大，无法通过经验进一步优化，我们称该模型**欠拟合**；如果模型的训练误差很小，但是测试误差很大，我们称该模型**过拟合**

一个简单的例子是**多项式回归**：当数据服从二次函数分布时，用线性函数去预拟合总是不能将训练误差降得相对较低，而使用 9 次多项式去拟合（可以求 `Moore-Penrose` 伪逆得到最优解析解）虽然能够将训练误差降得很低，但在测试集上的测试误差会变得非常高，只有我们选择二次多项式去拟合才能得到训练误差和泛化误差都最小的解

### 容量

由此引入一个概念 —— 模型的**容量**，其描述的是模型能够表示的函数空间的大小。在上述例子中，多项式次数越高的模型，其容量越大

容量低的模型容易发生欠拟合，容量较高的模型通常容易发生过拟合，只有适中的容量才能表现出较好的泛化能力

但是容量的选择显然不存在有 “最优解”，一定与实际任务有关：no free lunch theorem 告诉我们，在**所有数据生成分布**的维度下，任何机器学习模型的平均性能都是相等的，即不存在有万能的机器学习模型，但是对于特定的数据生成分布来说，机器学习算法是有优劣之分的

### 正则化

如何修改模型以找到更优的模型呢？除了改变假设空间，也即增减模型使用的函数种类和函数数量，我们还可以设定对模型参数的“偏好”

通过在训练误差的表达式中加入**正则项**，如在上述多项式回归的例子中，我们向代价函数中加入权重衰减的正则化项： $J(\boldsymbol{\omega})=MSE_{train}+\lambda\boldsymbol{\omega}^T\boldsymbol{\omega}$，$\lambda=0$ 表示我们没有任何偏好，$\lambda$ 越大，代表我们偏好权重越小的模型

正则化是一种更一般的修改模型容量的方式，移除某个函数可以视为我们对这个函数加了无限偏好（使得这个函数只有不存在才能最小化代价函数）

## 超参数与验证集

前述的多项式回归问题中函数个数、函数最高次数以及正则项系数均无法或很难通过 “学习” 找出最优解，这些参数我们称为**超参数**

确定这些参数的最优解是很困难且没有必要的，在实际任务中我们通常从训练集中随机抽取一组数据（20%左右）作为**验证集**，然后选定几组超参数，用验证集来测试超参数的性能以辅助我们选择合适的超参数

当数据量较小时，我们可以使用 k - 折交叉验证的方式来得到测试误差的无偏估计或渐进无偏估计，但是无法得到测试误差方差的无偏估计，且通常会低估测试误差的方差

## 估计、偏差和方差

### 点估计

使用样本的函数（也即统计量）作为参数的估计，这种方法称为**点估计**，前提是假设待估计参数 $\boldsymbol{\theta}$ 是固定未知的（与后续的 **Bayes 估计**区分）

$$\boldsymbol{\hat{\theta}} = g(\bold{X_1,X_2,...,X_n})$$

点估计并没有要求 g(·) 是一个表现良好的函数，实际上它可以是任意的函数，但一个好的点估计需要 g(·) 的输出接近实际参数 $\boldsymbol{\theta}$

### 偏差

一个估计的偏差定义为：$Bias(\boldsymbol{\hat{\theta}})=\mathbb{E}[\boldsymbol{\hat{\theta}}]-\boldsymbol{\theta}$

若一个估计的 Bias 为 0，我们称它为 $\boldsymbol{\theta}$ 的一个**无偏估计**，否则称它为有偏的

### 方差

方差衡量的是估计量的稳定性

定义样本均值: $\mu_m = \frac{1}{m}\sum \limits_{i=1}^m x^{(i)}$

#### 样本方差估计

$$\hat{\sigma}_m^2=\frac{1}{m}\sum \limits_{i=1}^m (x^{(i)}-\hat{\mu_m}^2)$$

可以证明上述 $\hat{\sigma}_m^2$ 是总体方差的有偏估计（咋证？）

#### 无偏样本方差估计

$$\tilde{\sigma}_m^2=\frac{1}{m-1}\sum \limits_{i=1}^m (x^{(i)}-\hat{\mu_m}^2)$$

上述无偏样本方差估计是对样本方差估计的修正

方差的平方根称为标准差 (standard error)，记为 $SE(\boldsymbol{\hat{\theta}})=\sqrt{Var(\boldsymbol{\hat{\theta}})}$，但是无论我们使用的 $Var(\boldsymbol{\hat{\theta}})$ 是否为无偏估计，$SE(\hat{\theta})$ 都会**低估真实的标准差** (Jessen 不等式)

均值的标准差：$SE(\hat{\mu}_m)=\sqrt{Var(\frac{1}{m}\sum \limits_{i=1}^m x^{(i)})}=\frac{\sigma}{\sqrt{m}}$

**用均值的标准差可以衡量均值的置信区间**，若均值服从 $N(\hat{\mu}_m,Var(\frac{1}{m}\sum \limits_{i=1}^m x^{(i)}))$，那么其 95% 置信区间为 $(\hat{\mu}_m-1.96SE(\hat{\mu}_m),\hat{\mu}_m+1.96SE(\hat{\mu}_m))$

> 一般称算法 A 的性能优于算法 B，若 A 测试误差均值 95% 置信区间的上界低于 B 测试误差均值 95% 置信区间的下界

### 一致性

称 $\hat{\theta}_m$ 依概率收敛到 $\theta$：$\plim\limits_{m\rightarrow\infty}\hat{\theta}_m=\theta$，若 $\forall \epsilon>0,\ \lim\limits_{m\rightarrow\infty}P(|\hat{\theta}_m-\theta|<\epsilon)=1$

一致性保证了估计量的偏差会随着样本数量的增加而减小；但是反过来是不对的，渐进无偏不代表一致性：如对于正态分布的均值，我们总是选择第一个样本作为估计量，这样得到的虽然是无偏估计，但不满足一致性
