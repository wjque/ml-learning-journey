# 监督学习算法

**监督学习**的基本设定：

- 给定一组训练数据：  
  $$
  \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \dots, (x^{(n)}, y^{(n)})\}
  $$
  其中：
  - $ x^{(i)} \in \mathbb{R}^d $ 是输入（特征），
  - $ y^{(i)} $ 是输出（标签），可以是连续值（回归）或离散值（分类）。

- 目标：学习一个函数 $ f: x \mapsto y $，使得对新的输入 $ x $，能准确预测 $ y $。

## 概率监督学习

一般的监督学习算法拟合的是一个映射 $f: \bold{x} \mapsto y$，给定 $\bold{x}$，我们需要预测可能的 $y$ 值

但是概率监督学习做的工作是拟合输出 $y$ 关于输入 $\bold{x}$ 的分布 $p(y|\bold{x};\boldsymbol{\theta})$，然后我们利用这个分布得到相应 $y$ 的估计值 (取数学期望或者 $\argmax\limits_y\ p(y|\bold{x};\boldsymbol{\theta})$)

### 线性回归与概率监督学习

在线性回归问题中，我们可以假设输出与输入服从的关系为：$y = \boldsymbol{\theta}^T\bold{x} + \epsilon$，其中 $\epsilon\sim\mathcal{N}(0,\sigma^2)$，$\sigma$ 为常量，那么 $y|\bold{x};\boldsymbol{\theta}\sim\mathcal{N}(\boldsymbol{\theta}^T\bold{x},\sigma^2),\therefore p(y|\bold{x};\boldsymbol{\theta})\propto\exp\{-\frac{(y - \boldsymbol{\theta}^T\bold{x})^2}{2\sigma^2}\}$

我们使用最大似然估计来获得 $\boldsymbol{\theta}$ 的估计值：$\boldsymbol{\theta} = \argmax\limits_{\boldsymbol{\theta}} \prod\limits_{i=1}^m p(y_i|\bold{x}_i;\boldsymbol{\theta})=\argmax\limits_{\boldsymbol{\theta}} \sum\limits_{i=1}^m \log p(y_i|\bold{x}_i;\boldsymbol{\theta})=\argmax\limits_{\boldsymbol{\theta}} \sum\limits_{i=1}^m -\frac{(y_i - \boldsymbol{\theta}^T\bold{x}_i)^2}{2\sigma^2}=\argmin\limits_{\boldsymbol{\theta}} \sum\limits_{i=1}^m(y_i-\boldsymbol{\theta}^T\bold{x}_i)^2$，可见最大化似然函数等价于最小化均方误差

得到 $\boldsymbol{\theta}$ 的估计值后，通常认为 $\hat{y}=\boldsymbol{\theta}^T\bold{x}$，也就是取 $y|\bold{x};\boldsymbol{\theta}$ 的数学期望

## 支持向量机

支持向量机所做的工作是将样本进行一个**二分类**，具体通过寻找一个超平面对样本点进行分割，并且这个超平面需位于**两类样本点最大间隔**的中间

实际中按照由简到繁的情况，依次有不同的算法解决：

- 训练样本线性可分，可以使用**硬间隔最大化**，学习一个**线性可分支持向量机**
- 训练样本近似线性可分，可以使用**软间隔最大化**，学习一个**线性支持向量机**
- 训练样本线性不可分，使用**核技巧**和**软间隔最大化**，学习一个**非线性支持向量机**

![线性分类](.\pics\线性分类.png)

## k-Nearest

## 决策树
