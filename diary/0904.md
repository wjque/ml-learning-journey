# Deep Reinforcement Learning

强化学习的基本步骤：

- 定义一个 `Function`：$Action = f(Observation)$，一般是一个神经网络
- 定义一个评价 `Reward` 的方法，用于优化 `Function` 生成的 `Action`
- 定义一个进行 `Optimization` 的方法

## Optimization

### Policy Gradient

优化方法需要考虑**时序关系**，越早的动作会对之后的结果都有影响，同时也要考虑时序的权重（discount），经历的时间越长，对之后的结果影响越小

越早的动作倾向获得越大的 `Reward`，这其实也是合理的，因为越早的动作越重要

`Reward` 是相对的，最后分析时需要减去一个 `baseline`，相当于做归一化处理

- 初始化 `Actor network` 参数
- 对于每一轮训练：
  - 使用**现有的参数**与环境互动，得到训练数据 $\{\{s_1, a_1\},\{s_2, a_2\},...\}$
  - 使用定义的评价方式得到每个动作的 `Reward` $\{A_1, A_2,...\}$
  - 使用梯度下降法更新 `Actor network` 的参数

用自身参数训练的方法称为 `On-Policy` 方法，用其他参数模型生成的数据训练的方法称为 `Off-Policy` 方法，**经典的 `Off-Policy` 方法就包括 `PPO` 算法**

### PPO

#### Importance Sampling

假设我们有一个无法显示得到的分布 $p(x)$，随机变量 $X$ 服从这个分布，我们希望计算 $f(x)$ 的数学期望 $\mathbb{E}_{x\sim p(x)}[f(x)]$，有两种方法：

- Monte Carlo
  - 从 $p(x)$ 中随机生成若干的 $x_1,x_2,...,x_n$，计算 $\frac{1}{n}\sum_{i=1}^n f(x_i)$
- Importance Sampling
  - 随机选择一个 $q(x)$，由于 $\mathbb{E}_{x\sim p(x)}[f(x)] = \int f(x)p(x)dx = \int f(x)\frac{p(x)}{q(x)}q(x)dx = \mathbb{E}_{x\sim q(x)}[f(x)\frac{p(x)}{q(x)}]$，所以我们可以从 $q(x)$ 中采样 $x$ 来计算 $f(x)\frac{p(x)}{q(x)}$ 的数学期望，从而得到 $\mathbb{E}_{x\sim p(x)}[f(x)]$

#### PPO算法

我们实际上希望使用参数为 $\theta'$ 的模型生成的数据来训练参数为 $\theta$ 的模型，避免以往 `On-Policy` 方法每次更新参数都要重新生成数据的问题

不同参数的模型实际上可以理解为不同的 `state-action` 分布，自变量 $x$ 是 `state`，因变量 f(x) 为 `acton`，我们希望用参数为 $\theta'$ 的模型生成的 `(state, action)` 数据来更新参数为 $\theta$ 的模型的策略梯度 $\nabla f(x)$，或者更准确地说，我们希望用这笔数据估计参数为 $\theta$ 的模型会得到多少 `reward / advantage`

- 初始化策略参数 $\theta^0$
- 对每一轮训练：
  - 使用 $\theta^k$ 与环境交互生成数据 $\{s_t,a_t\}$，并计算其 `reward / advantage` $A^{\theta^k}(s_t, a_t)$；原来的 $A^{\theta}(s_t, a_t)$ 每轮都要重新计算，现在可以使用 $A^{\theta^k}(s_t, a_t)$ 来估计一部分参数的 `reward`
  - 原来的目标是优化 $\sum p_{\theta}(a_t|s_t)A^{\theta}(s_t, a_t)$，现在的目标是优化 $J_{PPO}^{\theta^k}(\theta)=J^{\theta^k}(\theta)-\beta KL(\theta, \theta^k)$，其中 $J^{\theta^k}(\theta) = \sum \frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}A^{\theta^k}(a_t|s_t)$

### Actor-Critic

`Value Function` $V^{\theta}$：估计由所给的参数 $\theta$，在某一个 `Observation` s 下将得到的 discounted cumulated reward $V^{\theta}(s)$

估计 $V^{\theta}(s)$ 的方法：

- Monte Carlo (MC)
  - 使用某一个参数 $\theta$，实际与环境交互，计算实际得到的 `Reward` $G'$，这个数据用于训练 $V^{\theta}(s)$
- Temporal-difference (TD)
  - 仅使用 $s_t$ 到 $s_{t+1}$ 的 `Reward`，使用关系：$V^{\theta}(s_t) = \gamma V^{\theta}(s_{t+1}) + r_{t}$
