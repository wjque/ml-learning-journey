# Reinforcement Learning

强化学习的基本步骤：

- 定义一个 `Function`：$Action = f(Observation)$
- 定义一个评价 `Reward` 的方法，用于优化 `Function` 生成的 `Action`
- 定义一个进行 `Optimization` 的方法

## Optimization

### Policy Gradient

优化方法需要考虑**时序关系**，越早的动作会对之后的结果都有影响，同时也要考虑时序的权重（discount），经历的时间越长，对之后的结果影响越小

越早的动作倾向获得越大的 `Reward`，这其实也是合理的，因为越早的动作越重要

`Reward` 是相对的，最后分析时需要减去一个 `baseline`，相当于做归一化处理

- 初始化 `Actor network` 参数
- 对于每一轮训练：
  - 使用得到的参数与环境互动，得到训练数据 $\{\{s_1, a_1\},\{s_2, a_2\},...\}$
  - 使用定义的评价方式得到每个动作的 `Reward` $\{A_1, A_2,...\}$
  - 使用梯度下降法更新 `Actor network` 的参数

上述方法称为 `On-Policy` 方法，除此之外**经典的 `Off-Policy` 方法就包括 `PPO` 算法**

### Actor-Critic

`Value Function` $V^{\theta}$：估计由所给的参数 $\theta$，在某一个 `Observation` s 下将得到的 discounted cumulated reward $V^{\theta}(s)$

估计 $V^{\theta}(s)$ 的方法：

- Monte Carlo (MC)
  - 使用某一个参数 $\theta$，实际与环境交互，计算实际得到的 `Reward` $G'$，这个数据用于训练 $V^{\theta}(s)$
- Temporal-difference (TD)
  - 仅使用 $s_t$ 到 $s_{t+1}$ 的 `Reward`，使用关系：$V^{\theta}(s_t) = \gamma V^{\theta}(s_{t+1}) + r_{t}$
